\chapter{Evaluation}

After implementing XDTools, we conducted a user study. The goal of the study was to evaluate the suitability and quality of XDTools. Thus, we designed multiple tasks that the participants had to complete. As a baseline, the participants also completed some tasks without XDTools, using only the browser debugging tools available in Google Chrome. In the following sections, we will describe the setup of the study, present our results and conclude with a discussion of the results.

\section{Setup}

The study was carried out in a room of the GlobIS group. The participants were sitting in front of a desktop PC with a 30-inch (2560x1600 pixels) and had access to an English (US) keyboard and a mouse. The desktop PC was running Microsoft Windows 7 with Google Chrome Version 45. Participants also had access to two other devices: An Asus Nexus 7 (2012 version) and an HTC M9 Android phone. Furthermore, they were given a tutorial sheet about JavaScript that contained some functions that are useful for DOM modification as well as arrays. Figure~\ref{fig:study_setup} shows a picture of the setup of the study.

\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\textwidth]{images/study_setup2.png}
	\caption[Photo: Study setup]{Study Setup}
	\label{fig:study_setup}
\end{figure}

During the study, the instructor was sitting next to the participants and was available to answer any questions that occurred. If a participant did not make any progress on a task for some time, the instructor gave some hints to lead the participant in the right direction.

The participants had access to Chrome DevTools for all tasks. Furthermore, we set up multiple profiles on Chrome that the participants could use for emulating multiple devices. The participants could also remote debug the two real devices they had access to. The devices were already connected to the desktop PC by cable and the appropriate tab was opened in the browser. Thus, the participants only had to navigate to the tab and click on "inspect" to remote debug a device. 

During the study, we disabled some features of XDTools that were not required for completing the tasks so the participants would not be overwhelmed with the large amount of features that they had to learn how to use. We disabled the following features:
\begin{itemize}
	\item Changing the URL: The URL was given by the task anyway and it would have been of no use to change it during the study.
	\item Saving and loading device configurations: This feature is mainly useful for long-term use of XDTools and was not needed for completing the tasks in the study.
	\item Record and replay: We think that record and replay takes some time to get used to it and it is also only of limited use for simple tasks like the ones that we used in the study. Furthermore, it might distract participants from the actual task because they might want to try it out even if it does not help them to complete the task.
	\item Inspecting HTML: The tasks in our study did not require modification or debugging of HTML, thus we deactivated this feature.
	\item Settings: The settings were disabled for the study because they were implicitly given by us.
\end{itemize}

All other features could be used by the participants. The following list provides an overview of the features the participants had access to:
\begin{itemize}
	\item Emulating multiple devices
	\item Connecting real devices
	\item Connection features (auto-connect and drop-down list to connect to other devices)
	\item Shared JavaScript console
	\item Function debugging
	\item Shared CSS Editor
\end{itemize}
This list also represents the features that we wanted to evaluate during the study.

\subsection{Participants}

We recruited 12 participants (2 female) that all were university members in the department of computer science at ETH Zurich. Most participants were either PhD or Master students, but there were also some Bachelor students. The age of the participants ranged from 23 to 33 and the median age was 26. It was required that all participants have at least basic knowledge about front-end web technologies (i.e. HTML, CSS, JavaScript). However, the definition of "basic" was up to the participants and we did not ask participants to prove their experience before the study. Consequently, we also had some participants that had rather low experience with web technologies. We did not require participants to have any experience with cross-device application development as otherwise it would have been difficult to find enough participants. Nevertheless, two thirds of the participants actually did have some cross-device application development experience. 

\subsubsection{Previous Experience}
We asked all participants about their previous experience with web application development and JavaScript in particular, as well as about their previous experience with responsive web application development and cross-device web application development. Furthermore, we asked them about whether they have used Chrome DevTools before and how they used it. The participants rated their skills in web application development and JavaScript on a 5-point scale (see Figure~\ref{fig:xp}) from basic to proficient and also gave the numbers of years in experience (see Figure~\ref{fig:years_of_xp}) that they had in web application development and JavaScript. Even though only few participants had more than 5 years of experience, all participants rated their skills as rather good.

\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\textwidth]{images/charts/xp.pdf}
	\caption[Previous experience of participants]{Previous experience}
	\label{fig:xp}
\end{figure}

\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\textwidth]{images/charts/years_of_xp.pdf}
	\caption[Years of experience]{Years of experience}
	\label{fig:years_of_xp}
\end{figure}

Nine out of the twelve participants stated that they already had some experience with developing responsive web applications. All except one used browser tools for emulating devices for testing their applications and all except two used real devices. Only one participant stated that they only used real devices for testing and no browser tools. One participant mentioned that they mainly re-sized browser windows to test their applications.

Eight participants already had some experience with cross-device application development. Again, most of them used browser tools for emulating devices and/or real devices for testing their applications. Four of them either used multiple browsers, multiple browser profiles or incognito mode for emulating multiple devices on one device. The fact that the other participants did not use any of those ways of preventing the sharing of local resources indicates that they probably used multiple devices at all times. This could either be because the participants do not know that they exist, because they are inconvenient to use or because the participants simply prefer real devices.

Most of the participants already had experience with Chrome DevTools, only two participants indicated that they had never used them before and one of them had used the debugging tools of Firefox instead. We asked participants how often they used certain features of Chrome DevTools, in particular Device Mode, HTML and CSS inspection, JavaScript debugging and the console (see Figure~\ref{fig:devtools_xp}). The participants did not use Device Mode that often, which is no surprise, given that it is a rather new feature. All participants stated that they often use HTML and CSS inspection, thus this seems to be the most popular feature. The console was also used rather often. Surprisingly, JavaScript debugging was not that popular, less than half of the participants stated that they often use it.

\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\textwidth]{images/charts/devtools_xp.pdf}
	\caption[Previous experience with Chrome DevTools]{Previous experience with Chrome DevTools}
	\label{fig:devtools_xp}
\end{figure}

\subsection{Tasks}

We used two different applications for the tasks, namely the two sample applications that we described earlier, XDCinema and XDYouTube. For each application, there were two tasks; one was about finding and fixing a bug in the source code, and the other one was about implementing a new feature for the application (the features were also included in our final version of the sample applications). The maximum time for completing the tasks where the participants had to fix a bug was 15 minutes and the maximum time for implementing a feature was 30 minutes. After this time, we aborted the task unless it was clear that the participants would finish within the next 2 to 3 minutes. Each participant had to complete all four of the tasks; the tasks of one application with XDTools, the tasks of the other application without it. The order of the applications as well as whether the first two tasks were with or without XDTools was counter-balanced. The participants completed the debugging task first and then the implementation task. Thus, the participants could learn how to use the application during the debugging task, which we considered easier than the implementation task, and were already familiar with the application for the implementation task. The complete task descriptions that were given to the participants can be seen in~\ref{sec:tasks}. In the following, we give a summary of the tasks.

\subsubsection{XDYouTube}
XDYouTube allows users to use their personal devices to search for videos and add them to a queue. The videos from the queue are played one after the other on the largest of the devices. Users can also see the title and description of the currently playing video as well as the videos that are still in the queue by switching their device into landscape mode.

The first task (xdyt-bug) with XDYouTube was to fix a bug concerning the video queue. As soon as one video finishes playing, the next video is dequeued from the queue and starts playing. However, when no video is in the queue, a JavaScript error occurs and causes the next video that is added to the queue not to play. The participants were given a description of the task and then had to reproduce and fix it.  The participants were given a JavaScript file with two functions, one that adds a video to the queue and one that loads the next video at the end of a video.

For the second task (xdyt-impl), we asked participants to implement a remote control that can play and pause the current video from the controller devices. The participants had to implement two functions: One is called when the remote control button is clicked (the button as well as the event handler were already implemented), the other one is called when a shared variable that states whether the video is paused or playing is changed. Thus, the participants had to change the shared variable whenever the button was clicked and react accordingly on all devices if the shared variable changes, i.e. they had to pause or play the video on the device that plays the video and they had to change the text of the remote control button from "Pause" to "Play" and vice versa on all other devices. Furthermore, they had to change the CSS of the button such that it looked similar to a picture of a button that was given to them. The participants were given a JavaScript file with the two empty functions as well as a CSS file with the empty CSS selector of the button. Furthermore, they had access to a few helper functions that simplified the task.

\subsubsection{XDCinema}

XDCinema allows users to search for a city and date on one device. The device then shows a list of movies that play in this city on that date as well as the cinemas where the movie is played and the time that the movie starts. If the user clicks on a cinema, a summary of the movie as well as other information about the movie is shown on another device. If the user clicks on a cinema, the location of the cinema is shown on another device.

The first task (xdc-bug) was to fix a bug where the location of most cinemas was displayed wrongly, even though the data from the database was correct. The bug was that in one function, the variable "j" was used instead of "i", which caused a wrong location to be returned. The participants were given a JavaScript file with a few functions related to getting and updating the location on all devices.

In the second task (xdc-impl), the participants first had to complete the implementation of a function that shows the prices of each cinema in the selected city where the movie plays below the description of the cinema. A skeleton for this function was already given where a loop over all cinemas that show the movie was already implemented, the participants only had to fill in the body of the loop. The second part of the task was to highlight the correct price (the participants were given a CSS class called "highlighted") when the user clicks on a cinema in the search view and to improve the CSS for highlighting. In the original version, highlighting used a light grey background color and a white text color which was not very readable. The participants were given a JavaScript file with the two functions as well as CSS file with the initial CSS of the "highlighted" class. As in the XDYouTube implementation task, the participants also had access to some helper functions.

\subsection{Evaluation Methods}

In total, we used four different methods for evaluating the results of the user study. During the study, the participants had to fill out multiple questionnaires. Most our results are based on those questionnaires, the other methods (video recording, personal feedback, time measuring) for evaluation are only used for clearing any inconsistencies, finding explanations for things that cannot be explained with the questionnaires alone and gathering some additional information that we missed during the study.

\subsubsection{Questionnaires}
At the beginning of the study, each participant had to fill out a questionnaire about their background information. The results of this questionnaire were already presented earlier. After each task, the participant had to fill out another questionnaire with the following questions:
\begin{itemize}
	\item It was easy to complete the task with the tools I had access to.
	\item I felt efficient completing the task with the tools I had access to.
	\item It was challenging to complete the task with the tools I had access to.
	\item The tools I had access to were well suited for completing the task.
\end{itemize}
The questions could be answered on a 5-level Likert scale from "Strongly Disagree" to "Strongly Agree". In the tasks where the participants had access to XDTools, we also asked them to rate the usefulness of the individual features of XDTools on a 5-level Likert scale. The following question was asked about the features: "How useful did you find the following features for completing the task?". The participants could answer this question for each feature on a 5-level scale from "Not useful" to "Very useful". They could also choose the option "Not used" for features that they did not use while completing the task. For each task, there also was a comment field where the user could write down any additional comments that they had about the task or the tools they had access to.

After completing all tasks, the participants had to fill out a final questionnaire where they could answer some questions that compare XDTools to the browser debugging tools that they had access to. The participants could state whether it was easier and felt more efficient to debug and implement with or without XDTools. They could also state whether they preferred implementing and debugging with or without XDTools.

In addition, they also answered some general questions regarding the usability of XDTools, whether they think that XDTools is useful and whether they would use it for implementing and debugging cross-device applications. 

Finally, the participants could state which features of XDTools they would use for debugging and implementing cross-device applications and they could also write some comments about XDTools if they wanted to.

The complete set of questions can be seen in the questionnaires in~\ref{sec:questionnaires}.

\subsubsection{Video Recording}
During the study, we used a video camera to record the participants while completing the tasks. This was mainly done to make sure that no important information was lost and for extracting some task completion strategies from the videos.

\subsubsection{Personal Feedback}
At the end of the study, participants were encouraged to share any comments that they still wanted to mention and to give their general opinion about XDTools. Any comments that the participants had mentioned during the study were also noted.

\subsubsection{Time Measuring}
For each participant, the time required for completing each task was measured. This was mainly done to detect any major discrepancies between completion times with and without XDTools. However, exact times are not considered relevant for evaluation because they highly depend on the participant and on the hints given by the instructor during the study.

\section{Results}

In the following sections, we will first present the results from the individual tasks. For each task, we will compare how participants answered the questions in the per-task questionnaires with and without access to XDTools. 

\subsection{XDCinema: Fixing a Bug}

The results for the task where the participants had to fix a bug in XDCinema can be seen in Figure~\ref{fig:xdc_bug_comparison}. The figure shows the median values for the questions asked after the task with and without XDTools. For the question that asks about how challenging it was to complete the task with the tools the participant had access to, a lower value is better; for all other questions, a higher value is considered better. There is a rather big difference in the median value for the suitability of the tools, while the differences are smaller for the other questions or not there at all. Thus, it seems that even though the participants felt that XDTools is more suited for the task than the standard browser debugging tools, there is no significant difference regarding efficiency or easiness. However, this is not surprising, as the suitability of the tools is generally independent of the task, whereas the efficiency and easiness depend on the task. 

\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\textwidth]{images/charts/xdc_bug_comparison.pdf}
	\caption[xdc-bug: Comparison]{XDCinema debugging task - Comparison}
	\label{fig:xdc_bug_comparison}
\end{figure}

Figure~\ref{fig:xdc_bug_features_used} shows how many participants used the individual features of XDTools and how useful they found them. Obviously, the figure only includes the six participants that had access to XDTools. None of the participants used real devices and all of them used device emulation instead. However, two participants rated device emulation with a 3, which indicates that they found it somewhat useful for the task, but not extremely useful. The other participants all found device emulation very useful. The connection features and function debugging were used by all except one participant and were well appreciated by the participants. The shared JavaScript console was rather unpopular for this task, probably due to the simplicity of the task and also due to the fact that the bug produced no JavaScript errors that would be displayed in the console.

\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\textwidth]{images/charts/xdc_bug_features_used.pdf}
	\caption[xdc-bug: Features used]{XDCinema debugging task - Features used and their Usefulness}
	\label{fig:xdc_bug_features_used}
\end{figure}

\subsection{XDCinema: Implementing a Feature}

In Figure~\ref{fig:xdc_impl_comparison}, the results for the implementation task in XDCinema can be seen. In this task, the difference in suitability is less pronounced than in the XDCinema debugging task, but the difference for the question about efficiency and easiness is larger. In general, the task was considered as rather easy independent of the tools the participant had access to, although participants that had access to XDTools perceived it as a bit easier. The median value of five with XDTools suggests that participants had no problems completing the task at all when they had access to XDTools. Similar results can be observed concerning efficiency. The difference in median values suggests that the participants felt a bit more efficient with XDTools. Surprisingly, if we compare the average and median completion times for this task, the participants that had access to XDTools were considerably slower (comparing median values, the participants that had access to XDTools were about 9 minutes slower). In fact, this is the only task where the difference in completion times with and without XDTools is noticeable; the completion times for all other tasks are almost equivalent. However, those two facts do not necessarily contradict each other, as there were participants with very different experience levels and due to the low number of participants, such factors can easily have a big influence on completion times. In general, the completion times should not be considered as especially relevant, after all the instructor also gave some hints during the study and this can also distort completion times significantly. However, it would be interesting to see how completion times differ if there is a larger number of participants and if the participants had to complete tasks completely on their own. 

\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\textwidth]{images/charts/xdc_impl_comparison.pdf}
	\caption[xdc-impl: Comparison]{XDCinema implementation task - Comparison}
	\label{fig:xdc_impl_comparison}
\end{figure}

Figure~\ref{fig:xdc_impl_features_used} again shows the use and the ratings of the individual features. Again, no participant used the real devices. All participants used device emulation and the connection features and in contrast to the debugging task in XDCinema, device emulation was rated as very useful by all participants. Function debugging was used a bit less than in the debugging task, but the shared JavaScript console was used much more. It makes sense that function debugging is used more when fixing a bug; if one implements a feature and it works immediately when testing it, there is no need to debug a function, but if one has to fix a bug, there obviously must be a bug in a function and thus it makes much more sense to debug functions. The shared JavaScript console was rarely used to send commands and most participants did not use logging for solving the task, but many participants had some syntax errors when first testing the feature and noticed the error messages in the console. Finally, the CSS editor was also used by some participants in this task. However, some participants completed the CSS part of the task using only the CSS file. This may be because they did not think of the CSS editor at this specific moment, or because they know CSS so well that they can just write everything down immediately, or also because they do not consider the CSS editor useful for this task.

\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\textwidth]{images/charts/xdc_impl_features_used.pdf}
	\caption[xdc-impl: Features used]{XDCinema implementation task - Features used and their usefulness}
	\label{fig:xdc_impl_features_used}
\end{figure}

\subsection{XDYouTube: Fixing a Bug}

Figure~\ref{fig:xdyt_bug_comparison} shows the results for the debugging task in XDYouTube. The difference in suitability between XDTools and the usual browser tools was most significant in this task with a difference of 2. The difference in efficiency is also rather large. However, the task was rated as almost equally easy and challenging with and without XDTools despite the large differences in the other questions. Thus, XDTools seems to mainly influence the efficiency for this task. 

\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\textwidth]{images/charts/xdyt_bug_comparison.pdf}
	\caption[xdyt-bug: Comparison]{XDYouTube debugging task - Comparison}
	\label{fig:xdyt_bug_comparison}
\end{figure}

In Figure~\ref{fig:xdyt_bug_features_used}, the use and ratings of the individual features can be seen. All of the participants used device emulation and connection features and rated them as useful. Function debugging was also used by almost all participants, probably because it was difficult to reproduce the bug and the participants wanted to see what was going on in the functions. About half the participants used the shared JavaScript console, mainly to see the error produced in the function that caused the bug. One participant connected the Nexus 7 to XDTools and liked the feature, but no statement about the general usefulness of the feature can be made from just one participant. 

\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\textwidth]{images/charts/xdyt_bug_features_used.pdf}
	\caption[xdyt-bug: Features used]{XDYouTube debugging task - Features used}
	\label{fig:xdyt_bug_features_used}
\end{figure}

\subsection{XDYouTube: Implementing a Feature}

Figure~\ref{fig:xdyt_impl_comparison} shows the results for the implementation task in XDYouTube. In this task, all questions were clearly rated in favor of XDTools. While the participants answered the questions in a rather neutral way when they did not have access to XDTools, they clearly stated that the task was easy to complete and felt efficient to complete with XDTools. This task differs from the others a bit, because in all other tasks the difference in median values was rather insignificant for some questions, whereas the difference is at least 1 in this task for every question. Thus, it seems that XDTools makes the most difference for this task regarding all aspects.

\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\textwidth]{images/charts/xdyt_impl_comparison.pdf}
	\caption[xdyt-impl: Comparison]{XDYouTube implementing task - Comparison}
	\label{fig:xdyt_impl_comparison}
\end{figure}

In Figure~\ref{fig:xdyt_impl_features_used}, the use and ratings of the individual features can be seen. Once again, device emulation and the connection features were used by every participant. The shared JavaScript console and CSS editor were about equally popular and rated as very useful except for one participant found the console only partially useful. Function debugging was used by half the participants and rated as very useful.

\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\textwidth]{images/charts/xdyt_impl_features_used.pdf}
	\caption[xdyt-impl: Features used]{XDYouTube implementing task - Features used}
	\label{fig:xdyt_impl_features_used}
\end{figure}

\section{Discussion}

Figure~\ref{fig:implementing_easier} shows that about three quarters of all participants considered implementing a feature easier with XDTools. Only one participant found it easier to implement a feature without XDTools. However, in principle, this option is redundant as the participants still have access to the browser debugging tools even when they have access to XDTools. This essentiall means that about one quarter of the participants did not see any gain in easiness from using XDTools. The same applies to the question about whether implementing a feature feels more efficient with XDTools (see Figure~\ref{fig:implementing_efficient}); this figure shows exactly the same results as the figure about easiness. However, all except one participant preferred implementing a feature with XDTools (see Figure~\ref{fig:prefer_implementing}). Thus, it seems that participants like to have access to XDTools even if they cannot directly relate it to a decrease in difficulty or to an increase in efficiency.
\begin{figure}[H]
  \centering
    \includegraphics[width=0.6\textwidth]{images/charts/implementing_easier.pdf}
	\caption[Easiness of implementing]{Easiness of implementing a feature}
	\label{fig:implementing_easier}
\end{figure}

\begin{figure}[H]
  \centering
    \includegraphics[width=0.6\textwidth]{images/charts/implementing_efficient.pdf}
	\caption[Efficiency of implementing]{Efficiency of implementing a feature}
	\label{fig:implementing_efficient}
\end{figure}

\begin{figure}[H]
  \centering
    \includegraphics[width=0.6\textwidth]{images/charts/prefer_implementing.pdf}
	\caption[Preference for implementing]{Preference for implementing a feature}
	\label{fig:prefer_implementing}
\end{figure}

Figure~\ref{fig:debugging_easier} shows that most participants found it easier to debug a cross-device application with XDTools. The results get even more obvious if we look at Figure~\ref{fig:debugging_efficient} and Figure~\ref{fig:prefer_debugging}. Those two figures show that all participants felt more efficient when debugging with XDTools and also preferred debugging with XDTools.

\begin{figure}[H]
  \centering
    \includegraphics[width=0.6\textwidth]{images/charts/debugging_easier.pdf}
	\caption[Easiness of debugging]{Easiness of debugging}
	\label{fig:debugging_easier}
\end{figure}

\begin{figure}[H]
  \centering
    \includegraphics[width=0.6\textwidth]{images/charts/debugging_efficient.pdf}
	\caption[Efficiency of debugging]{Efficiency of debugging}
	\label{fig:debugging_efficient}
\end{figure}

\begin{figure}[H]
  \centering
    \includegraphics[width=0.6\textwidth]{images/charts/prefer_debugging.pdf}
	\caption[Preference for debugging]{Preference for debugging}
	\label{fig:prefer_debugging}
\end{figure}


We also asked the participants if they would use XDTools for debugging and implementing cross-device applications and if they think that XDTools would be useful for cross-device application testing. The results can be seen in Figure~\ref{fig:usefulness_tool}. Almost all participants think that XDTools would be very useful for implementing as well as debugging cross-device applications and the remaining few also think that they would be useful. All participants would use XDTools for implementing cross-device applications and all except one participant would use them for debugging cross-device applications. A few more participants stated that they strongly agree with the statement that they would use XDTools for debugging than for implementing. This is consistent with our previous results, where all participants preferred debugging with XDTools but about one quarter did not prefer implementing with XDTools.

\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\textwidth]{images/charts/usefulness_tool.pdf}
	\caption[Usefulness]{Usefulness of XDTools}
	\label{fig:usefulness_tool}
\end{figure}

Finally, the results for the general questions about XDTools can be seen in Figure~\ref{fig:tool_general}. The figure shows that XDTools was perceived as easy to learn despite the many different features and the participants also felt rather confident using XDTools. However, some participants rated the question about confidence in a neutral way and most other participants felt only somewhat confident, but not very confident. This indicates that even though XDTools is considered easy to learn, it may still take participants some time to get used to it. XDTools are not considered as unnecessarily complex by most of the participants. The results of those three questions indicate that the interface of XDTools is generally well-structured and it would be easy for developers to get used to XDTools.

\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\textwidth]{images/charts/tool_general.pdf}
	\caption[General Evaluation]{General evaluation of XDTools}
	\label{fig:tool_general}
\end{figure}

Figure~\ref{fig:would_use_features} shows which features the participants would use for implementing and debugging cross-device applications. In general, participants would rather use emulated devices than real devices for both implementing and debugging. This is understandable as most things can be done just as well with emulated devices as with real devices. Some participants mentioned that they would not use real devices during implementing, but that they would test their application on real devices after finishing implementing to make sure the application works fine on them. The connection features are almost unavoidable to use, thus it is surprising that some participants state that they would not use them. However, for some parts of debugging and implementing, one device might be sufficient for testing and no connection features would be required. One participant mentioned that the connection features seem very natural and that there is no point in asking about their usefulness because it is obvious that they are useful. This indicates that the feature was indeed greatly appreciated by some participants. The shared JavaScript console is equally popular for debugging and implementing and would be used by almost all participants, thus it seems to be a very popular feature as well. Function debugging is more popular for debugging than for implementing. This corresponds to the actual results of the study and has been elaborated before. Apart from connecting real devices, the shared CSS editor is the least popular. This may be due to the fact that browsers already have quite mature CSS editors and it might be possible to test CSS on one device at a time in many cases. Also, the CSS parts of our tasks were rather simple, thus the real value of such a feature might not be obvious to the participants. While things like changing the background color of a button can easily be done on just one device, more challenging CSS problems like positioning elements require more effort and look much different on different devices. Furthermore, cross-device applications do not always show the same things on all devices and applying CSS to all devices is of no use if the corresponding elements are only shown on one device anyways.

\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\textwidth]{images/charts/would_use_features.pdf}
	\caption[Features that participants would use]{Features that the participants would use}
	\label{fig:would_use_features}
\end{figure}

During the study, some participants mentioned that they found device emulation very useful because they can have all devices in one place instead of having to manage different browser windows and profiles. Reloading all devices at once was also considered as very useful. With multiple browser windows, each window has to be reloaded individually which makes this a much more tedious task. It was also appreciated that the devices are still connected after reloading, even though for some applications this is also the case with multiple browser windows. The participants really liked function debugging, the only criticism was that it does not show on which device a function is called. In general, the output aggregation of the shared JavaScript console was considered more useful than sending commands t multiple devices. One participant even mentioned that they would not use it to send commands, but the aggregation is very useful. 

In general, the feedback during the study showed that participants really liked XDTools. One participant stated that they think that the tool is very useful and they wish they had access to it when they were working on a cross-device application. Another participant mentioned that "The existing features are already very impressive and work well. They really help the developer working on cross-device applications."

Record and replay was disabled for the user study, but one participant that had attended a presentation about XDTools before, mentioned that they would find it immensely useful. They consider it as a powerful feature that could be very helpful for replicating bugs in an application and for regression testing. Often, it is not clear how to reach a bug and being able to record the set of interactions that lead to the bug and then replay them can simplify this process.

\subsubsection{Aborted Tasks}

Due to the time limits that we set, we also had to abort some tasks. There were five cases where we had to abort a task, four while participants had access to XDTools, and one where the participant did not have access to XDTools. There was one participant that finished all tasks except the XDCinema debugging task. The participant had access to XDTools for this task, but we do not consider this relevant for this specific case, as the time required for completing this task can vary greatly depending on how fast the participant notices the switched variable. Also, the participant had completed all other tasks successfully. Furthermore, there was one participant for whom we had to abort both implementation tasks (one with XDTools and one without) and one participant where we had to abort both XDCinema tasks (both with XDTools). Both those participants had very low experience in web application development and had problems completing basic things like placing brackets correctly around an if-statement and creating a new line in HTML. This indicates that those participants had very low programming experience in general and it is not surprising that they failed to complete some tasks. The participants also had problems completing the other tasks, but eventually managed to finish the tasks with a lot of help from the instructor. Thus, we do not think that the fact that most of the aborted tasks were carried out with XDTools says anything relevant about XDTools. 

\subsubsection{Devices Used}

Overall, the number of devices used did not vary greatly depending on the participant and on the condition the participant was in. Although our tasks were designed to require at least two devices in general, it was possible to complete the XDCinema debugging task with only one device, but the task required less device interaction when multiple devices were involved. Figure~\ref{fig:n_emulated} shows how many emulated devices the participants used on average with and without XDTools. The number of emulated devices used is slightly higher with XDTools for all tasks. One reason for this difference could be that it is easier to add a new device with XDTools. However, the difference is rather small and further investigation is needed to see if this is really the case. For the XDCinema debugging task, only two out of six participants that did not have access to XDTools used more than one device. With XDTools, four participants used more than one device. This also indicates that participants prefer to use more devices when they have access to XDTools. For the XDCinema implementation task, all participant used exactly three devices (when combining real and emulated devices) which is not surprising because the minimum number of devices required was three and additional devices did not provide any value. 

\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\textwidth]{images/charts/n_emulated.pdf}
	\caption[Emulated device used]{Average number of emulated devices used}
	\label{fig:n_emulated}
\end{figure}

The real devices were only rarely used overall. In the XDYouTube implementation and debugging tasks, one participant per condition used one real device. In the XDCinema implementation task, one participant that did not have access to XDTools used two real devices. Normally, most of the tasks required for implementing cross-device applications can be performed just as well on emulated devices. Testing on real devices is typically limited to points where major implementation steps have been completed or when a developer wants to see how interactions feel or work on actual touch devices. Therefore, it is no surprise that real devices were not used frequently during our study. The number of devices used during the tasks shows that although participants sometimes stick to the minimum number of devices required, they also like to be able to add more devices for convenience of further testing. In the XDYouTube implementation task, participants often added three devices so they could put one in landscape mode and one in portrait mode. This allowed them to add videos to the queue and test their implementation of the remote control button without switching orientation in-between. 

In terms of screen sizes used, participants mostly used low-resolution mobile phones and sometimes tablets when carrying out tasks with XDTools. The devices were in portrait mode at almost all times, except for when they were required to be in landscape mode by the application. This is probably due to the dimensions of the area where devices can be placed which makes it more convenient to have devices in portrait mode. Furthermore, most predefined devices are in portrait mode by default and participants did not switch their orientation unless required to. Without access to XDTools, participants also often used smaller devices, but sometimes they had one large full-screen browser window and other smaller browser windows that they switched to when required. Participants that used three devices often let one device fill the left half of the screen and let the other two devices fill one quarter of the screen each, which put those devices in landscape mode. 

\subsubsection{Debugging and Implementation Strategies}

In both debugging tasks and both conditions, about half the participants started by looking at the code and about half started by trying to reproduce the bug. Most of the participants switched between the code and the application multiple times while carrying out the tasks, although some looked at the code directly in the DevTools. Almost all participants used the DevTools at some point for completing the task, and most of the participants that had access to XDTools used function debugging. The debugging task in XDCinema and XDYouTube had some fundamental differences: The main difficulty for the XDYouTube debugging task was reproducing the bug, once the bug was reproduced, the cause of the bug was pretty obvious given the JavaScript error the bug produced. In contrast, reproducing the bug in the XDCinema debugging task was trivial, but finding out what causes the bug was more difficult. In XDYouTube, participants usually spent quite a lot of time trying to reproduce the bug before they switched to the DevTools or function debugging. Only after participants failed at reproducing the bug, they used tools for debugging in an attempt to find the bug this way. In XDCinema, participants switched to the DevTools or function debugging much faster because they reproduced the bug almost immediately. Consequently, in XDYouTube, much more time is spent switching between the code and application because the participants try out different ways of finding the bug. In XDCinema, the debugging process is much more streamlined because after reproducing the bug, participants mostly stick to either looking at the code or using tools for debugging.

In the implementation task, some participants started by looking for the HTML element that contained the prices or the remote control button first, while others started implementing right away. All participants implemented everything required for the XDYouTube implementation task at once and only then started testing it. Only the CSS was usually added after making sure that the button works. On one hand, it makes sense to implement the complete remote control functionality at once because if only part of it was implemented, it would be difficult to see if the implemented function actually works. On the other hand, we would have expected that at least some participants implemented part of the functionality and used logging mechanisms to see if it works. In the XDCinema implementation task, the task was clearly separated into to parts and the first part did not require the second part to work. Thus, all except one participant implemented the first part, tested it, and then moved on to the second part. However, almost all participants had to modify their first part a little bit while implementing the second part because of the different requirements. The CSS part of the task was again implemented after completing everything else by most participants.

In summary, no real difference in implementation and debugging strategies can be seen between the two conditions of our study. The only difference is that some parts of the debugging process are made more efficient or easier because the developer has access to additional tools. Thus, developers should be able to use the same strategies for debugging and implementing features as they usually would and augment them with additional features provided by XDTools where it is useful. 

\subsubsection{Feature Requests}

During the study, some participants suggested some improvements to existing features that would help them even more. Multiple participants suggested that function debugging should show on which device a function is called. We were able to successfully implement this feature after completing the user study. Also, one participant said that auto-connect should be enabled by default because there rarely is a situation where a developer would not want to connect the devices. Over the course of the study, it became obvious that this feature would be useful as many participants forgot to enable auto-connect before adding more devices and thus either manually connected them or removed them, enabled auto-connect, and re-added the devices. Furthermore, it was suggested that the border of the device should be in the device's unique color so devices can more easily be recognized. Those two features were also included into XDTools after the study. Also, it became clear during the study that it was not at all obvious when function debugging was working and when the participants had to re-open the DevTools. Due to this, we now show a warning when the developer should re-open the DevTools to make sure that function debugging works. Also, we now display a warning message when the user wants to reload the page because some participants accidentally reloaded the page instead of the devices and then lost all their devices. Some additional feature suggestions include:
\begin{itemize}
	\item CSS value suggestions, e.g. suggest "1px solid black" when the developer sets the border property
	\item More integration with the DevTools
	\item Opening the DevTools automatically when the developer wants to debug a function
	\item Auto-complete or similar for functions in function debugging
	\item Combining a source code editor with XDTools
\end{itemize}
Due to time constraints and in some cases also feasibility constraints we were not able to implement those feature requests (yet). Also, while some of those requests like CSS value suggestions would clearly be useful, others like combining a source code editor with XDTools require more research to determine if they are actually desired by developers and would provide some additional benefits.